{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6417d1da",
   "metadata": {},
   "source": [
    "# RAG System Implementation - Academic Research Assistant\n",
    "\n",
    "This notebook implements a complete Retrieval-Augmented Generation (RAG) system designed for academic research workflows. The system combines local language models with a structured knowledge base to provide transparent and reliable AI-assisted research capabilities.\n",
    "\n",
    "## System Overview\n",
    "\n",
    "The RAG pipeline consists of several integrated components:\n",
    "\n",
    "1. **Document Processing**: Intelligent chunking of academic texts with section-aware splitting and citation tracking\n",
    "2. **Embedding Generation**: High-quality semantic representations using BGE-base-en-v1.5\n",
    "3. **Vector Search**: Efficient similarity search using FAISS for document retrieval\n",
    "4. **Response Generation**: Local LLM inference with proper source attribution\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Local Inference**: Privacy-preserving processing using llama.cpp\n",
    "- **Citation Tracking**: Automatic extraction and linking of academic references\n",
    "- **Source Attribution**: Transparent identification of information sources\n",
    "- **Modular Design**: Easy customisation and extension of components\n",
    "- **Academic Focus**: Optimised for research workflows and scholarly content\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. **Setup**: Install dependencies from requirements.txt\n",
    "2. **Model Configuration**: Update LLM model path in the final cells\n",
    "3. **Document Processing**: Run cells sequentially to build the knowledge base\n",
    "4. **Query Processing**: Use the final cells to test queries and generate answers\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "The system demonstrates practical implementation of:\n",
    "- Semantic chunking with overlap preservation\n",
    "- Dense vector retrieval with metadata tracking\n",
    "- Structured prompt engineering for academic contexts\n",
    "- Local LLM integration for controlled generation\n",
    "\n",
    "*Note: This implementation focuses on dynamic pricing research content but can be adapted for other academic domains.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to have the venv as python interpreter\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d24d3",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felix/Desktop/RAG_project/RAG2/rag_project2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for RAG system implementation\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Text chunking utilities\n",
    "from sentence_transformers import SentenceTransformer  # Embedding model for semantic search\n",
    "import faiss  # Vector database for similarity search\n",
    "import numpy as np  # Numerical operations\n",
    "from sklearn.preprocessing import normalize  # Data preprocessing utilities\n",
    "from llama_cpp import Llama  # Local LLM inference engine\n",
    "from striprtf.striprtf import rtf_to_text  # RTF text extraction\n",
    "import re  # Regular expressions for text processing\n",
    "from typing import List, Dict  # Type hints for better code clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference metadata manually entered\n",
    "# This dictionary stores citation information for academic papers used in the thesis\n",
    "# Each entry contains the citation key, formatted citation, title, and source link\n",
    "references = [\n",
    "    {\n",
    "        \"id\": \"(Chen & Guestrin, 2016)\",\n",
    "        \"citation\": \"(Chen and Guestrin, 2016)\",\n",
    "        \"title\": \"XGBoost: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\",\n",
    "        \"link\": \"https://dl.acm.org/doi/10.1145/2939672.2939785\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"(Fiig et al., 2018)\",\n",
    "        \"citation\": \"(Fiig et al., 2018)\",\n",
    "        \"title\": \"Dynamic Pricing of Airline Offers\",\n",
    "        \"link\": \"https://www.iata.org/contentassets/0688c780d9ad4a4fadb461b479d64e0d/dynamic-pricing--of-airline-offers.pdf\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"(Garbarino & Lee, 2003)\",\n",
    "        \"citation\": \"(Garbarino and Lee, 2003)\",\n",
    "        \"title\": \"Dynamic Pricing in Internet Retail: Effects on Consumer Trust\",\n",
    "        \"link\": \"https://www.researchgate.net/publication/229740251_Dynamic_Pricing_in_Internet_Retail_Effects_on_Consumer_Trust\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"(Spagnuelo et al., 2017)\",\n",
    "        \"citation\": \"(Spagnuelo et al., 2017)\",\n",
    "        \"title\": \"Metrics for Transparency\",\n",
    "        \"link\": \"https://core.ac.uk/download/pdf/78370936.pdf\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a citation dictionary for fast lookups during text processing\n",
    "# This enables the system to identify and link citations found in the thesis text\n",
    "citation_index = {\n",
    "    entry[\"citation\"]: {\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"title\": entry[\"title\"],\n",
    "        \"link\": entry[\"link\"]\n",
    "    }\n",
    "    for entry in references\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a3747",
   "metadata": {},
   "source": [
    "### Functions used for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_citation_keys(text: str, citation_index: Dict[str, Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract academic citation keys from text using regex patterns.\n",
    "    \n",
    "    This function identifies citation patterns like \"(Author, Year)\" and matches them\n",
    "    against the citation index to find valid references.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for citations\n",
    "        citation_index (Dict[str, Dict]): Dictionary mapping citation keys to metadata\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of valid citation keys found in the text\n",
    "    \"\"\"\n",
    "    # Find all citation patterns in the format \"(Author, Year)\"\n",
    "    matches = re.findall(r\"\\(([^()]+?,\\s?\\d{4})\\)\", text)\n",
    "    # Filter matches to only include valid citations from our index\n",
    "    return [m.strip() for m in matches if m.strip() in citation_index]\n",
    "\n",
    "def split_by_section_headings_with_meta(\n",
    "    text: str,\n",
    "    source_type: str,\n",
    "    source_name: str = \"\",\n",
    "    citation_index: Dict[str, Dict] = None,\n",
    "    max_chunk_words: int = 200,\n",
    "    overlap_words: int = 40\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split text into semantic chunks based on section headings with metadata preservation.\n",
    "    \n",
    "    This function identifies numbered section headers (e.g., \"1 Introduction\", \"2.1 Method\")\n",
    "    and splits the text accordingly. Each section is further divided into smaller overlapping\n",
    "    chunks to maintain semantic coherence while fitting within LLM context windows.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked\n",
    "        source_type (str): Type of source (\"thesis\" or \"reference\")\n",
    "        source_name (str, optional): Name of the source document\n",
    "        citation_index (Dict[str, Dict], optional): Citation lookup dictionary\n",
    "        max_chunk_words (int): Maximum words per chunk (default: 200)\n",
    "        overlap_words (int): Number of overlapping words between chunks (default: 40)\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of chunk dictionaries with metadata including:\n",
    "            - section_title: The section heading\n",
    "            - text: The chunk content\n",
    "            - type: Source type (thesis/reference)\n",
    "            - source: Source name (for references)\n",
    "            - citations: List of citations found (for thesis chunks)\n",
    "    \"\"\"\n",
    "    # Match section headers like \"1 Introduction\" or \"2.1 Method\"\n",
    "    # This regex identifies numbered sections at the beginning of lines\n",
    "    headings = list(re.finditer(r\"(?<=\\n)(\\d{1,2}(?:\\.\\d{1,2})?)\\s+([A-Z][^\\n]*)\", text))\n",
    "    chunks = []\n",
    "\n",
    "    # Process each section identified by headings\n",
    "    for i, match in enumerate(headings):\n",
    "        section_title = match.group().strip()\n",
    "        start = match.end()\n",
    "        # Determine section end (next heading or end of text)\n",
    "        end = headings[i + 1].start() if i + 1 < len(headings) else len(text)\n",
    "        content = text[start:end].strip()\n",
    "\n",
    "        # Break content into smaller overlapping semantic chunks\n",
    "        # This sliding window approach maintains context while controlling chunk size\n",
    "        words = content.split()\n",
    "        pointer = 0\n",
    "        \n",
    "        while pointer < len(words):\n",
    "            # Extract chunk of maximum size\n",
    "            sub_chunk_words = words[pointer:pointer + max_chunk_words]\n",
    "            sub_text = \" \".join(sub_chunk_words).strip()\n",
    "\n",
    "            if sub_text:\n",
    "                # Create chunk with metadata\n",
    "                chunk = {\n",
    "                    \"section_title\": section_title,\n",
    "                    \"text\": sub_text,\n",
    "                    \"type\": source_type\n",
    "                }\n",
    "\n",
    "                # Add source information for reference documents\n",
    "                if source_type == \"reference\":\n",
    "                    chunk[\"source\"] = source_name\n",
    "\n",
    "                # Extract and store citations for thesis chunks\n",
    "                if source_type == \"thesis\" and citation_index:\n",
    "                    chunk[\"citations\"] = extract_citation_keys(sub_text, citation_index)\n",
    "\n",
    "                chunks.append(chunk)\n",
    "\n",
    "            # Move pointer forward with overlap for context preservation\n",
    "            pointer += max_chunk_words - overlap_words  # sliding window\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b3042",
   "metadata": {},
   "source": [
    "### Load the texts using the functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading utilities\n",
    "# These functions handle loading and preprocessing of various document formats\n",
    "\n",
    "def txt_to_plain_text(txt_path):\n",
    "    \"\"\"\n",
    "    Convert a TXT file to plain text string.\n",
    "    \n",
    "    Args:\n",
    "        txt_path (str): Path to the text file\n",
    "        \n",
    "    Returns:\n",
    "        str: Plain text content with leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        plain_text = file.read()\n",
    "    return plain_text.strip()\n",
    "\n",
    "# Load the different source documents\n",
    "# These include the main thesis and supporting reference papers\n",
    "thesis = txt_to_plain_text(\"thesis/fyp_thesis.txt\")\n",
    "\n",
    "# Load reference papers that support the thesis research\n",
    "fiig_reference = txt_to_plain_text(\"RAG_sources_cleaned/Dynamic_Pricing_of_Airline_Offers.txt\")\n",
    "garbarino_reference = txt_to_plain_text(\"RAG_sources_cleaned/Dynamic_Pricing_in_Internet_Retail_OCR.txt\")\n",
    "chen_reference = txt_to_plain_text(\"RAG_sources_cleaned/XGBoost_A_Scalable_Tree_Boosting_System copy.txt\")\n",
    "spagnuelo_reference = txt_to_plain_text(\"RAG_sources_cleaned/Metrics_for_Transparency.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37870a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document chunking process\n",
    "# Convert each document into structured chunks with metadata\n",
    "# This enables granular retrieval and proper source attribution\n",
    "\n",
    "# Process the main thesis document\n",
    "# Include citation tracking to maintain academic integrity\n",
    "thesis_chunks = split_by_section_headings_with_meta(\n",
    "    text=thesis,\n",
    "    source_type=\"thesis\",\n",
    "    citation_index=citation_index\n",
    ")\n",
    "\n",
    "# Process reference documents\n",
    "# Each reference is tagged with its citation key for proper attribution\n",
    "fiig_chunks = split_by_section_headings_with_meta(\n",
    "    text=fiig_reference,\n",
    "    source_type=\"reference\",\n",
    "    source_name=\"(Fiig et al., 2018)\"\n",
    ")\n",
    "\n",
    "garbarino_chunks = split_by_section_headings_with_meta(\n",
    "    text=garbarino_reference,\n",
    "    source_type=\"reference\",\n",
    "    source_name=\"(Garbarino & Lee, 2003)\"\n",
    ")\n",
    "\n",
    "chen_chunks = split_by_section_headings_with_meta(\n",
    "    text=chen_reference,\n",
    "    source_type=\"reference\",\n",
    "    source_name=\"(Chen & Guestrin, 2016)\"\n",
    ")\n",
    "\n",
    "spagnuelo_chunks = split_by_section_headings_with_meta(\n",
    "    text=spagnuelo_reference,\n",
    "    source_type=\"reference\",\n",
    "    source_name=\"(Spagnuelo et al., 2017)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b660db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Combine all document chunks into a single corpus\n",
    "# This creates a unified knowledge base for the RAG system\n",
    "total_chunks = thesis_chunks + fiig_chunks + garbarino_chunks + chen_chunks + spagnuelo_chunks\n",
    "\n",
    "# Extract text content for embedding generation\n",
    "# The embedding model requires plain text input\n",
    "chunks = [chunk[\"text\"] for chunk in total_chunks]\n",
    "\n",
    "# Initialize the embedding model\n",
    "# BGE-base-en-v1.5 is a high-quality English embedding model\n",
    "# suitable for semantic search and retrieval tasks\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Generate embeddings for all text chunks\n",
    "# This converts text into dense vector representations for similarity search\n",
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector database setup using FAISS\n",
    "# FAISS (Facebook AI Similarity Search) provides efficient similarity search\n",
    "# IndexFlatL2 uses L2 (Euclidean) distance for exact nearest neighbor search\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Add embeddings to the index\n",
    "# Convert to float32 as required by FAISS\n",
    "index.add(np.array(embeddings).astype(np.float32))\n",
    "\n",
    "# Store chunk metadata for result retrieval\n",
    "# This lookup table maps vector indices to original chunk metadata\n",
    "chunk_lookup = total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Title: 2.2\tProblem Outline\n",
      "Text: 3rd party to benefit from dynamic pricing strategies.\n",
      "\n",
      "Section Title: 1.3\tDynamic Pricing in Retail\n",
      "Text: to market demand and sell products at the highest price customers are willing to pay. Secondly, it allows retailers to have efficient inventory management by having them adjust their prices based on surplus or scarcity of stock. Finally, dynamic pricing can boost customer satisfaction and trust through fair prices aligned with market conditions. Nevertheless, dynamic pricing, especially in retail is facing significant criticism, as some consumers find it unfair (Garbarino & Lee, 2003) to change prices for different conditions. For instance, car-service apps created a surge in price with their drivers in New York City during peak hours. This led to legal prosecutions by regulators to avoid such excess. Dynamic pricing can also have severe consequences for the company if it is misused. This can lead to a decrease of sales or brand reputation. This is especially true with various website or navigation add-ons emerging to alert customers of price changes (e.g. Honey) Industry leaders like JD (JD Corporate Administrator, 2021) and Amazon make use of sophisticated dynamic pricing mechanisms, demonstrating the growing interest in this new innovation. For instance, Amazon reported an increase in its profits by 25% by changing prices as frequently as 2.5 million times a day.\n",
      "\n",
      "Section Title: 8\tConclusion\n",
      "Text: of the market dynamics, considering both predictions and actual market changes. While dynamic pricing is a great tool for revenue optimisation, its success in the retail sector will depend on the development of solutions that are not only technologically advanced but also transparent and user-friendly, making retailers confident when implementing these solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval function for finding relevant document chunks\n",
    "def retrieve_relevant_chunks(query, embed_model, index, chunk_lookup, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant document chunks for a given query.\n",
    "    \n",
    "    This function implements semantic search by:\n",
    "    1. Encoding the query into an embedding vector\n",
    "    2. Searching the vector index for similar chunks\n",
    "    3. Returning the metadata of the most relevant chunks\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question or search query\n",
    "        embed_model: The sentence transformer model for encoding\n",
    "        index: The FAISS vector index\n",
    "        chunk_lookup: List of chunk metadata for result mapping\n",
    "        top_k (int): Number of most relevant chunks to return\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of chunk dictionaries with metadata\n",
    "    \"\"\"\n",
    "    # Encode the query into an embedding vector\n",
    "    query_embedding = embed_model.encode([query])\n",
    "    \n",
    "    # Search the vector index for similar chunks\n",
    "    # D contains distances, I contains indices of nearest neighbors\n",
    "    D, I = index.search(np.array(query_embedding).astype(np.float32), top_k)\n",
    "    \n",
    "    # Return the metadata of the most relevant chunks\n",
    "    return [chunk_lookup[i] for i in I[0]]\n",
    "\n",
    "# Example query demonstrating the retrieval system\n",
    "query = \"What are the benefits of dyamic pricing in retail?\"\n",
    "\n",
    "# Retrieve relevant chunks for the query\n",
    "# This demonstrates the core retrieval functionality\n",
    "relevant_chunks = retrieve_relevant_chunks(\n",
    "    query=query,\n",
    "    embed_model=model,\n",
    "    index=index,\n",
    "    chunk_lookup=chunk_lookup,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# Display the retrieved chunks with their metadata\n",
    "# This shows how the system finds and presents relevant information\n",
    "for chunk in relevant_chunks:\n",
    "    print(f\"Section Title: {chunk['section_title']}\")\n",
    "    print(f\"Text: {chunk['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb720db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response generation and context formatting functions\n",
    "# These functions create structured prompts for the LLM with proper source attribution\n",
    "\n",
    "def format_context_with_citation_keys(chunks, citation_index):\n",
    "    \"\"\"\n",
    "    Format retrieved chunks with proper source attribution and citation information.\n",
    "    \n",
    "    This function creates a structured context string that clearly identifies:\n",
    "    - Whether content comes from the thesis or reference papers\n",
    "    - Which specific references are cited within thesis content\n",
    "    - Full titles of reference papers for transparency\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): Retrieved chunks with metadata\n",
    "        citation_index (Dict): Citation lookup dictionary\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted context string with source attribution\n",
    "    \"\"\"\n",
    "    formatted_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source_type = chunk[\"type\"]\n",
    "        text = chunk[\"text\"]\n",
    "\n",
    "        if source_type == \"thesis\":\n",
    "            # For thesis content, identify any citations referenced\n",
    "            cited_keys = chunk.get(\"citations\", [])\n",
    "            if cited_keys:\n",
    "                # Create a readable list of cited sources\n",
    "                citation_labels = \", \".join(\n",
    "                    citation_index[cid][\"citation\"] for cid in cited_keys if cid in citation_index\n",
    "                )\n",
    "                formatted = f\"(From Thesis, citing {citation_labels})\\n{text}\"\n",
    "            else:\n",
    "                formatted = f\"(From Thesis)\\n{text}\"\n",
    "\n",
    "        elif source_type == \"reference\":\n",
    "            # For reference content, show the full paper title\n",
    "            citation_key = chunk.get(\"source\", \"\")\n",
    "            citation_info = citation_index.get(citation_key, {})\n",
    "            ref_title = citation_info.get(\"title\", citation_key)\n",
    "            formatted = f\"(From Reference: {ref_title})\\n{text}\"\n",
    "\n",
    "        else:\n",
    "            # Fallback for unknown source types\n",
    "            formatted = f\"(From Unknown)\\n{text}\"\n",
    "\n",
    "        formatted_chunks.append(formatted)\n",
    "\n",
    "    return \"\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "\n",
    "def build_prompt(query, retrieved_chunks, citation_index):\n",
    "    \"\"\"\n",
    "    Build a structured prompt for the LLM with context and instructions.\n",
    "    \n",
    "    This function creates a comprehensive prompt that includes:\n",
    "    - Clear instructions for the LLM's role\n",
    "    - Properly formatted context with source attribution\n",
    "    - The user's query\n",
    "    - Guidelines for citation and reference handling\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        retrieved_chunks (List[Dict]): Relevant chunks from retrieval\n",
    "        citation_index (Dict): Citation lookup dictionary\n",
    "    \n",
    "    Returns:\n",
    "        str: Complete prompt for LLM processing\n",
    "    \"\"\"\n",
    "    # Format context with proper source attribution\n",
    "    context = format_context_with_citation_keys(retrieved_chunks, citation_index)\n",
    "    \n",
    "    # Create structured prompt with clear instructions\n",
    "    return f\"\"\"[INST] You are an academic assistant helping to summarise and explain the content of a thesis.\n",
    "\n",
    "Use the following context to answer the question. If the context comes from a reference or citation, clearly state which reference it is from (e.g. Fiig et al., 2018). Otherwise, assume it's from the thesis itself.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "\n",
    "def generate_rag_answer(query, embed_model, index, chunk_lookup, citation_index, llm):\n",
    "    \"\"\"\n",
    "    Generate a complete RAG (Retrieval-Augmented Generation) answer.\n",
    "    \n",
    "    This function orchestrates the entire RAG pipeline:\n",
    "    1. Retrieve relevant chunks based on the query\n",
    "    2. Format context with proper source attribution\n",
    "    3. Build a structured prompt for the LLM\n",
    "    4. Generate the final answer using the local LLM\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        embed_model: Sentence transformer for encoding\n",
    "        index: FAISS vector index\n",
    "        chunk_lookup: Chunk metadata lookup\n",
    "        citation_index: Citation information\n",
    "        llm: Local LLM instance\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prompt, answer) - The full prompt and generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    retrieved = retrieve_relevant_chunks(query, embed_model, index, chunk_lookup)\n",
    "    \n",
    "    # Step 2: Build structured prompt\n",
    "    prompt = build_prompt(query, retrieved, citation_index)\n",
    "\n",
    "    # Step 3: Generate answer using local LLM\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.3, top_p=0.9)\n",
    "    answer = response['choices'][0]['text'].strip()\n",
    "    \n",
    "    return prompt, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a2738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/felix/Desktop/RAG_project/mistral-4bit.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 4165.48 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 5.44 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (Extreme Gradient Boosting) is a machine learning algorithm that builds upon gradient boosting techniques, which are used for additive optimization in functional space. It was first described in 2016 by Chen and Guestrin. XGBoost enhances the foundation of gradient boosting by introducing a regularized objective to prevent overfitting. The algorithm simplifies the formulation and optimizes it for parallel processing, making it efficient and scalable. Column sampling is another effective technique used in XGBoost to reduce overfitting and improve training efficiency. It is inspired by methods from Random Forests.\n",
      "\n",
      "In the thesis, XGBoost was applied to reduce the root mean squared error (RMSE) score of a time series forecasting model. The final model used was XGBoost, which had been tested because of its increasing popularity and success in major data science competitions. No hyperparameter tuning was made in the implementation of the XGBoost model, and the accuracy of 0.08 in the RMSE score was considered accurate enough.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12920.85 ms\n",
      "llama_print_timings:      sample time =     130.43 ms /   232 runs   (    0.56 ms per token,  1778.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =  998744.51 ms /   732 tokens ( 1364.41 ms per token,     0.73 tokens per second)\n",
      "llama_print_timings:        eval time =  268357.02 ms /   231 runs   ( 1161.72 ms per token,     0.86 tokens per second)\n",
      "llama_print_timings:       total time = 1269510.58 ms\n"
     ]
    }
   ],
   "source": [
    "# Complete RAG system demonstration\n",
    "# This example shows the full pipeline from query to answer\n",
    "\n",
    "# Example query about a specific technical concept\n",
    "query = \"Describe what is XGBoost and how is it applied in the thesis\"\n",
    "\n",
    "# Initialize the local LLM\n",
    "# Using llama.cpp for efficient local inference with a quantized model\n",
    "# Parameters:\n",
    "# - model_path: Path to the quantized model file\n",
    "# - n_ctx: Context window size (4096 tokens)\n",
    "# - n_threads: Number of CPU threads for inference\n",
    "# - n_batch: Batch size for processing\n",
    "llm = Llama(model_path=\"/Users/felix/Desktop/RAG_project/mistral-4bit.gguf\", n_ctx=4096, n_threads=4, n_batch=4)\n",
    "\n",
    "# Generate RAG answer using the complete pipeline\n",
    "# This demonstrates the integration of retrieval and generation\n",
    "prompt, answer = generate_rag_answer(\n",
    "    query=query,\n",
    "    embed_model=model,\n",
    "    index=index,\n",
    "    chunk_lookup=chunk_lookup,\n",
    "    citation_index=citation_index,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Display the generated answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b75a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are an academic assistant helping to summarise and explain the content of a thesis.\n",
      "\n",
      "Use the following context to answer the question. If the context comes from a reference or citation, clearly state which reference it is from (e.g. Fiig et al., 2018). Otherwise, assume it's from the thesis itself.\n",
      "\n",
      "Context:\n",
      "(From Reference: (Chen & Guestrin, 2016))\n",
      "builds upon a long line of research in gradient boosting and scalable machine learning systems. This section highlights how XGBoost relates to, and improves upon, previous work. Gradient boosting, originally developed as a general technique for additive optimisation in functional space, has been successfully applied to a wide variety of tasks including classification, learning to rank, and structured prediction. XGBoost enhances this foundation by introducing a regularised objective that helps prevent overfitting. While similar ideas appear in earlier works such as Regularized Greedy Forest (RGF), XGBoost simplifies the formulation and optimises it for parallel processing. Column sampling is another effective technique used in XGBoost to reduce overfitting and improve training efficiency. It is inspired by methods from Random Forests, where features are randomly selected at each split. By incorporating this into gradient boosting, XGBoost further stabilises training and enables better generalisation, particularly on high-dimensional datasets. Sparsity-aware learning has been explored extensively in linear models, but rarely in decision tree algorithms. Most tree learners either assume dense input or require specialised procedures for handling missing values or categorical variables. XGBoost is one of the first to provide a unified and principled approach to handling all kinds of sparsity, offering both accuracy\n",
      "\n",
      "(From Reference: (Chen & Guestrin, 2016))\n",
      "of magnitude larger than those used in typical benchmarks. Overall, XGBoost offers an efficient, scalable, and versatile framework for gradient boosting that bridges the gap between algorithmic innovation and real-world deployment. The lessons learned through its development provide useful insights that can inform the design of other high-performance machine learning systems.\n",
      "\n",
      "(From Thesis)\n",
      "explored to reduce this RMSE score. 4.3.3 XGBoost The final model used is Extreme Gradient Boosting (XGBoost), originally described in 2016 (Chen & Guestrin, 2016). It has first been tested because of its increasing popularity and success in major data science competitions. XGBoost relies on gradient-boosted trees, which are used to obtain universal function approximators. The RMSE score was 0.05 on the training data and 0.08 on the validation data. Having tested multiple datasets on XGBoost and SARIMAX, XGBoost always had a better RMSE score through various datasets, so we decided to use this model. No hyperparameter tuning (number of trees, maximum depth of the trees, learning rate and regularisation parameter) has been made in the implementation of the XGBoost model, the accuracy of 0.08 in the RMSE score was considered accurate enough.\n",
      "\n",
      "Question:\n",
      "Describe what is XGBoost and how is it applied in the thesis\n",
      "[/INST]\n",
      "XGBoost (Extreme Gradient Boosting) is a machine learning algorithm that builds upon gradient boosting techniques, which are used for additive optimization in functional space. It was first described in 2016 by Chen and Guestrin. XGBoost enhances the foundation of gradient boosting by introducing a regularized objective to prevent overfitting. The algorithm simplifies the formulation and optimizes it for parallel processing, making it efficient and scalable. Column sampling is another effective technique used in XGBoost to reduce overfitting and improve training efficiency. It is inspired by methods from Random Forests.\n",
      "\n",
      "In the thesis, XGBoost was applied to reduce the root mean squared error (RMSE) score of a time series forecasting model. The final model used was XGBoost, which had been tested because of its increasing popularity and success in major data science competitions. No hyperparameter tuning was made in the implementation of the XGBoost model, and the accuracy of 0.08 in the RMSE score was considered accurate enough.\n"
     ]
    }
   ],
   "source": [
    "# Display both the prompt and answer for transparency\n",
    "# This shows the complete interaction between the retrieval system and LLM\n",
    "\n",
    "# The prompt shows:\n",
    "# - Retrieved context with source attribution\n",
    "# - The user's original question\n",
    "# - Instructions for proper citation handling\n",
    "print(\"=== PROMPT ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== ANSWER ===\")\n",
    "# The answer demonstrates:\n",
    "# - How the LLM synthesizes information from multiple sources\n",
    "# - Proper citation and reference handling\n",
    "# - Coherent response generation based on retrieved context\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
